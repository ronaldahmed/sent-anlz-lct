%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Cross-domain Soft Patterns for Sentiment Analysis}

\author{Ronald Cardenas Acosta\\
  University of Malta \\
  {\tt ronald.cardenas.18@um.edu.mt} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This report describes experiments 
\end{abstract}


\section{Introduction}

what is domain adaptation

what is sent analizis

why is domain adp important for sent analisis

what we are introducing
	build upon ---

on what data, which domains, method, 
contributions?


\section{Related Work}

sopa does well on sent analisis on one-domain

how prev work struggled because of vocabulary mismatch

sopa's flexibility to match patterns with * or missing elements
--> chance to explicitly model vocab mismatch



\section{Domain Adaptation with Soft Patterns}

explain training proc here


\section{Experimental Setup}

We build upon the implementation of SoPa introduced by \citet{schwartz2018sopa}.\footnote{\url{https://github.com/Noahs-ARK/soft_patterns}} All models are implemented in PyTorch\footnote{\url{https://pytorch.org/}}.

\subsection{Dataset}

We use the provided dataset, a balanced subset of the reviews data extracted by \citet{mcauley2015image}. The data consists of users reviews on two domains --movies and TV, and games--, extracted from Amazon.
We use Movies \& TV category as source domain and Games as target domain. We extract a development subset from the source domain and further divide the target domain's data into unlabeled, development, and test splits. Table~\ref{table:data-splits} presents the sizes of each split considered in the experiments.

\begin{table*}[]
\centering
\begin{tabular}{|l|c|r|r|c|}
\hline
Domain              & Train                       & \multicolumn{1}{c|}{Dev} & \multicolumn{1}{c|}{Test} & Unlabeled                  \\ \hline
Movies \& TV (src) & \multicolumn{1}{r|}{89,998} & 17,999                   & 10,000                    & -                          \\ \hline
Games (tgt)         & -                           & 5,000                    & 11,142                    & \multicolumn{1}{r|}{5,000} \\ \hline
\end{tabular}
\caption{Size of data splits in source (src) and target (tgt) domains.}
\label{table:data-splits}
\end{table*}

\subsection{Training of source domain}

We use pre-trained 300-dimensional GloVe 840B embeddings \citet{pennington2014glove} normalized to unit length. Training was performed using Adam \cite{kingma2014adam} as optimizer.

For hyper-parameter tunning, we resort to a subset of the training and development source data, consisting of 10,000 and 5,000 instances, respectively. These subsets were sampled following a uniform distribution without replacement. We use a Tree-structured Parzen Estimator (TPE) optimization model over 30 iterations\footnote{We use HyperOpt library (\url{http://hyperopt.github.io/hyperopt/})}. Table~\ref{table:param-tuning} shows the range of hyper-paramter values explored and their optimal values.

\begin{table*}[]
\begin{tabular}{|l|r|r|}
\hline
\multicolumn{1}{|c|}{Hyper-parameter} & \multicolumn{1}{c|}{Range}                                                                       & \multicolumn{1}{c|}{Optimal} \\ \hline
Patterns                              & \begin{tabular}[c]{@{}r@{}}\{6:10, 5:10, 4:10, 3:10, 2:10\},\\ \{6:10, 5:10, 4:10\}\end{tabular} & \{6:10, 5:10, 4:10\}         \\ \hline
Learning rate                         & $10^{-9}$--$10^{-2}$                                         & 0.00015                      \\ \hline
Dropout                               & 0--0.2                                                                                           & 0.0017                       \\ \hline
MLP hid. dim.                         & 100--300                                                                                         & 100                          \\ \hline
Batch size                            & 10--64                                                                                           & 20                           \\ \hline
\end{tabular}
\caption{Range and optimal values of hyper-parameters tuned.}
\label{table:param-tuning}
\end{table*}


\subsection{Self-training of target domain}




\section{Results and Discussion}




\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2019}


\end{document}
