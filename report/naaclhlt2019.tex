%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Cross-domain Soft Patterns for Sentiment Analysis}

\author{Ronald Cardenas Acosta\\
  University of Malta \\
  {\tt ronald.cardenas.18@um.edu.mt} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This report describes experiments 
\end{abstract}


\section{Introduction}

%what is domain adaptation

The objective of domain adaptation techniques is to adapt a hypothesis trained on a source data distribution so that it can perform well on a related target distribution.
These techniques have been applied to a variety of NLP tasks such as sentiment analysis \cite{blitzer2007biographies,mcauley2013hidden,mcauley2015image,ruder2018strong}, style transfer in text generation \cite{fu2018style,NIPS2018_7959,peng-etal-2018-towards}, textual and visual question answering \cite{chao2018cross,zhao2018finding}, and machine translation \cite{etchegoyhen2018evaluating,britz2017effective}, to name a few.

In the case of sentiment analysis of online user reviews, previous work has sought effective ways of transfer learning between product categories \cite{blitzer2007biographies,ruder2018strong}. However, the task has been proven to be challenging since sentiment is expressed differently in different domains.
For instance, \citet{blitzer2007biographies} identifies three types of feature behaviour across domains: (a) features that are highly predictive in the source domain but not in the target domain, (b) features that are highly predictive in the target domain but not in the source domain, and (c) features that positively predictive in the source domain but negatively predictive in the target domain (or viceversa).

In this report, we focus on unsupervised domain adaptation for the task of sentiment analysis, transfering from a single source domain into a single target domain.
We build upon the recently proposed {\sc SoPa} \cite{schwartz2018sopa}, a neural architecture that mimic the behaviour of a Weighted Finite State Machine.
{\sc SoPa} is able to learn soft lexical patterns, i.e.\ word patterns that might include a (possibly empty) wild card.
We investigate the performance of {\sc SoPa} under a self-training setup following calibration procedures proposed by \citet{ruder2018strong}.
Experiments on Amazon online reviews of two product categires show promising results.

% what is sent analizis
% why is domain adp important for sent analisis
% what we are introducing
%	 build upon ---
% on what data, which domains, method, 
% contributions?


\section{Related Work}


sopa does well on sent analisis on one-domain

how prev work struggled because of vocabulary mismatch

sopa's flexibility to match patterns with * or missing elements
--> chance to explicitly model vocab mismatch



\section{Domain Adaptation with Soft Patterns}

explain training proc here


\section{Experimental Setup}

We build upon the implementation of SoPa introduced by \citet{schwartz2018sopa}.\footnote{\url{https://github.com/Noahs-ARK/soft_patterns}} All models are implemented in PyTorch\footnote{\url{https://pytorch.org/}}.

\subsection{Dataset}

We use the provided dataset, a balanced subset of the reviews data extracted by \citet{mcauley2015image}. The data consists of users reviews on two domains --movies and TV, and games--, extracted from Amazon.
We use Movies \& TV category as source domain and Games as target domain. We extract a development subset from the source domain and further divide the target domain's data into unlabeled, development, and test splits. Table~\ref{table:data-splits} presents the sizes of each split considered in the experiments.

\begin{table*}[]
\centering
\begin{tabular}{|l|c|r|r|c|}
\hline
Domain              & Train                       & \multicolumn{1}{c|}{Dev} & \multicolumn{1}{c|}{Test} & Unlabeled                  \\ \hline
Movies \& TV (src) & \multicolumn{1}{r|}{89,998} & 17,999                   & 10,000                    & -                          \\ \hline
Games (tgt)         & -                           & 5,000                    & 11,142                    & \multicolumn{1}{r|}{5,000} \\ \hline
\end{tabular}
\caption{Size of data splits in source (src) and target (tgt) domains.}
\label{table:data-splits}
\end{table*}

\subsection{Training of source domain}

We use pre-trained 300-dimensional GloVe 840B embeddings \citet{pennington2014glove} normalized to unit length. Training was performed using Adam \cite{kingma2014adam} as optimizer.

For hyper-parameter tunning, we resort to a subset of the training and development source data, consisting of 10,000 and 5,000 instances, respectively. These subsets were sampled following a uniform distribution without replacement. We use a Tree-structured Parzen Estimator (TPE) optimization model over 30 iterations\footnote{We use HyperOpt library (\url{http://hyperopt.github.io/hyperopt/})}. Table~\ref{table:param-tuning} shows the range of hyper-paramter values explored and their optimal values.

\begin{table*}[]
\begin{tabular}{|l|r|r|}
\hline
\multicolumn{1}{|c|}{Hyper-parameter} & \multicolumn{1}{c|}{Range}                                                                       & \multicolumn{1}{c|}{Optimal} \\ \hline
Patterns                              & \begin{tabular}[c]{@{}r@{}}\{6:10, 5:10, 4:10, 3:10, 2:10\},\\ \{6:10, 5:10, 4:10\}\end{tabular} & \{6:10, 5:10, 4:10\}         \\ \hline
Learning rate                         & $10^{-9}$--$10^{-2}$                                         & 0.00015                      \\ \hline
Dropout                               & 0--0.2                                                                                           & 0.0017                       \\ \hline
MLP hid. dim.                         & 100--300                                                                                         & 100                          \\ \hline
Batch size                            & 10--64                                                                                           & 20                           \\ \hline
\end{tabular}
\caption{Range and optimal values of hyper-parameters tuned.}
\label{table:param-tuning}
\end{table*}


\subsection{Self-training of target domain}




\section{Results and Discussion}




\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2019}


\end{document}
